{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18dc8a70",
   "metadata": {},
   "source": [
    "ðŸ“Œ Pre-processing for Sensor data\n",
    "\n",
    "Here we are going to represent step by step our processing of data for the human activities :\n",
    "\n",
    "ðŸ”¹ Walking\n",
    "\n",
    "ðŸ”¹ Running\n",
    "\n",
    "ðŸ”¹ Standing up\n",
    "\n",
    "ðŸ”¹ Sitting down\n",
    "\n",
    "ðŸ”¹ Climbing stairs\n",
    "\n",
    "\n",
    "\n",
    "The whole dedication includes the following steps:\n",
    "\n",
    "ðŸ”¹ Segmentation with overlap(20%)\n",
    "\n",
    "ðŸ”¹ Trimming \n",
    "\n",
    "ðŸ”¹ Normalization\n",
    "\n",
    "ðŸ”¹ Calculate magnitude\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62108031",
   "metadata": {},
   "source": [
    "STEP 1 - SPLIT & OVERLAP\n",
    "\n",
    "For the following activities (running and walking) we split our data in parts of 10 seconds with overlap each other up to 20%\n",
    "\n",
    "Note: The data we collect in those two phases(running and walking) was continously is parts of 1-1.5 minutes so we need to split it to smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e9905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def segment_session(session_folder, output_base, window_size=10, step_size=8):\n",
    "    \n",
    "    #Devides the signals Accelerometer, Gravity , Gyroscope into parts of 10 seconds with 20% overlap.\n",
    "    #Each part shifts by 8 seconds\n",
    "    \n",
    "    signal_files = ['Accelerometer.csv', 'Gravity.csv', 'Gyroscope.csv']\n",
    "    dfs = {}\n",
    "\n",
    "    # load of csv files and check if they exist\n",
    "    for signal in signal_files:\n",
    "        path = os.path.join(session_folder, signal)\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path)\n",
    "            if df.empty or 'seconds_elapsed' not in df.columns:\n",
    "                continue\n",
    "            dfs[signal] = df\n",
    "\n",
    "    #We use the minimum max time across all signals to ensure alignment\n",
    "    max_time = min(df['seconds_elapsed'].max() for df in dfs.values())\n",
    "    start = 0\n",
    "    segment_id = 0\n",
    "\n",
    "    #Repeat until the end of the session\n",
    "    while start + window_size <= max_time:\n",
    "        end = start + window_size\n",
    "\n",
    "        #create the folder for each segment\n",
    "        segment_folder = os.path.join(output_base, f\"segment_{segment_id+1:03d}\")\n",
    "        os.makedirs(segment_folder, exist_ok=True)\n",
    "\n",
    "        #save each signal segment in the corresponding folder\n",
    "        for signal, df in dfs.items():\n",
    "            segment_df = df[(df['seconds_elapsed'] >= start) & (df['seconds_elapsed'] < end)]\n",
    "            segment_df.to_csv(os.path.join(segment_folder, signal), index=False)\n",
    "\n",
    "        start += step_size\n",
    "        segment_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f92dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input folder containing raw sensor sessions\n",
    "base_folder = r\"C:\\Users\\Vasil\\DataAnalysisLearningMethodsAss1\\Data-SA\\run\"\n",
    "# Define the output folder for segmented sessions\n",
    "output_base = r\"C:\\Users\\Vasil\\DataAnalysisLearningMethodsAss1\\Data-SA\\run_segments\"\n",
    "\n",
    "# Loop through each session folder inside the 'run' directory\n",
    "\n",
    "for session_name in os.listdir(base_folder):\n",
    "    session_path = os.path.join(base_folder, session_name)\n",
    "    # Ensure it's a directory\n",
    "    if os.path.isdir(session_path):\n",
    "        segment_session(session_path, os.path.join(output_base, session_name), window_size=10, step_size=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1cc9cc",
   "metadata": {},
   "source": [
    "STEP 2 - TRIMMING\n",
    "\n",
    "For the following human activities (standing up, sitting down, climbing stairs) we used trimming to remove the first 1.5 and last 1.5 seconds of each measurement to ensure that we will not include the movement of the phone inside our pocket\n",
    "\n",
    "Note:The data we collect for the following phases(standing up, sitting down, climbing stairs) was in parts of 8 seconds max and in total 30 files so we didn't need to split them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_signals_in_folder(input_folder, output_folder, trim_seconds=1.5):\n",
    "    \n",
    "    #Trims the first and last 1.5 seconds from each signal file in the input folder\n",
    "\n",
    "    signal_files = ['Accelerometer.csv', 'Gravity.csv', 'Gyroscope.csv']\n",
    "    #Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    #Loop through each signal file\n",
    "    for signal_file in signal_files:\n",
    "        file_path = os.path.join(input_folder, signal_file)\n",
    "        #Check if the file exists\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            if df.empty or 'seconds_elapsed' not in df.columns:\n",
    "                continue\n",
    "                #Get the time range of the signal\n",
    "            min_time = df['seconds_elapsed'].min()\n",
    "            max_time = df['seconds_elapsed'].max()\n",
    "            #Skip trimming if the signal is too short\n",
    "            if max_time - min_time <= 2 * trim_seconds:\n",
    "                continue\n",
    "                #Keep only the rows within the trimmed time window\n",
    "            df = df[(df['seconds_elapsed'] >= min_time + trim_seconds) &\n",
    "                    (df['seconds_elapsed'] <= max_time - trim_seconds)]\n",
    "            #Save the new trimmed signal\n",
    "            df.to_csv(os.path.join(output_folder, signal_file), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba711e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input folder\n",
    "input_root = r\"C:\\Users\\Vasil\\DataAnalysisLearningMethodsAss1\\Data-SA\\sit\"\n",
    "\n",
    "# Define the output folder\n",
    "output_root = r\"C:\\Users\\Vasil\\DataAnalysisLearningMethodsAss1\\Data-SA\\sit_trimmed\"\n",
    "\n",
    "# Loop through each session folder inside 'sit'\n",
    "for folder_name in os.listdir(input_root):\n",
    "    input_path = os.path.join(input_root, folder_name)\n",
    "    output_path = os.path.join(output_root, folder_name)\n",
    "\n",
    "    # Check if the current item is a directory\n",
    "    if os.path.isdir(input_path):\n",
    "        # Apply trimming to remove first and last 1.5 seconds from each signal file\n",
    "        trim_signals_in_folder(input_path, output_path, trim_seconds=1.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8505950",
   "metadata": {},
   "source": [
    "STEP 3 - NORMALIZATION & MAGNITUDE\n",
    "\n",
    "For all the new files that we have created until now we normalize them and calculate magnitude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89617caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def normalize_and_add_magnitude(input_folder, output_folder):\n",
    "   \n",
    "   # Applies Min-Max Normalization to the x, y, z columns and computes the magnitude of the vector.\n",
    "\n",
    "\n",
    "    signal_files = ['Accelerometer.csv', 'Gravity.csv', 'Gyroscope.csv']\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    # Initialize the MinMaxScaler for normalization\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    for signal_file in signal_files:\n",
    "        file_path = os.path.join(input_folder, signal_file)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            #Skip if the dataframe is empty or missing required columns\n",
    "            if df.empty or not all(col in df.columns for col in ['x', 'y', 'z']):\n",
    "                continue\n",
    "            # Apply Min-Max scaling to x, y, z and compute magnitude\n",
    "            df[['x', 'y', 'z']] = scaler.fit_transform(df[['x', 'y', 'z']])\n",
    "            df['magnitude'] = np.sqrt(df['x']**2 + df['y']**2 + df['z']**2)\n",
    "            #Save the new processed DataFrame in output folder which includes normalized x, y, z and magnitude\n",
    "            df.to_csv(os.path.join(output_folder, signal_file), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eb4f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input-output folder pairs for preprocessing\n",
    "folders_to_process = {\n",
    "    # Folder with segmented 'run' data (each session contains multiple segment folders)\n",
    "    r\"C:\\Users\\Vasil\\DataAnalysisLearningMethodsAss1\\Data-SA\\run_segments\":\n",
    "        r\"C:\\Users\\Vasil\\DataAnalysisLearningMethodsAss1\\Data-SA\\run_processed\",\n",
    "\n",
    "    # Folder with trimmed 'sit' data (each folder contains sensor files directly)\n",
    "    r\"C:\\Users\\Vasil\\DataAnalysisLearningMethodsAss1\\Data-SA\\sit_trimmed\":\n",
    "        r\"C:\\Users\\Vasil\\DataAnalysisLearningMethodsAss1\\Data-SA\\sit_processed\"\n",
    "}\n",
    "\n",
    "# Loop through each input-output folder pair\n",
    "for input_root, output_root in folders_to_process.items():\n",
    "    for folder_name in os.listdir(input_root):\n",
    "        input_path = os.path.join(input_root, folder_name)\n",
    "        output_path = os.path.join(output_root, folder_name)\n",
    "\n",
    "        # Check if the current item is a directory \n",
    "        if os.path.isdir(input_path):\n",
    "\n",
    "            # Special case: run_segments contains nested folders (segment_001, segment_002, ...)\n",
    "            #Here we check if the processing data is included in run_segments because the earlier version failed\n",
    "            if \"run_segments\" in input_root:\n",
    "                for segment_name in os.listdir(input_path):\n",
    "                    segment_path = os.path.join(input_path, segment_name)\n",
    "                    output_segment = os.path.join(output_path, segment_name)\n",
    "\n",
    "                    # Only process if the segment folder exists\n",
    "                    if os.path.isdir(segment_path):\n",
    "                        # Apply normalization and magnitude calculation to each segment\n",
    "                        normalize_and_add_magnitude(segment_path, output_segment)\n",
    "\n",
    "            else:\n",
    "                # For sit_trimmed: sensor files are directly inside each folder\n",
    "                # Apply normalization and magnitude calculation directly\n",
    "                normalize_and_add_magnitude(input_path, output_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
